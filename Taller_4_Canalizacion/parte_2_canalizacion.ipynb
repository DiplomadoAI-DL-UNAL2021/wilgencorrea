{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Taller parte 2 \n","### Parte 1:\n","## Autor:Wilgen Correa\n","### Fecha; 30-04-2021\n","### Objetivo:Objetivo: Un cuaderno en donde ustedes harán el entubamiento \n","# de un conjunto de datos asociado a su proyecto. \n","# Para este, ustedes revisan el segundo cuaderno y completan \n","# detalles con los enlaces abajo. \n","# El entregable es preferiblemente el entubamiento que usted usa o usará en su proyecto."]},{"cell_type":"markdown","metadata":{},"source":[" ### Cargar librerias"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import datetime\n","import os, glob\n","import numpy as np\n","import seaborn as sns\n","import geopandas as gpd\n","import pandas as pd\n","import imageio\n","from pyspatialml import Raster\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.pipeline import Pipeline\n","import tensorflow as tf\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.models import Sequential\n","from sklearn.impute import SimpleImputer\n","from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n","from tensorflow.keras.callbacks import CSVLogger, RemoteMonitor\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n","import earthpy.plot as ep\n","import earthpy.spatial as es\n",""]},{"cell_type":"markdown","metadata":{},"source":["### Realizar el cargue de imágenes y las áreas de enetrenamiento\n","\n","Bandas a seleccionar de la imagen de entrada de SENTINEL-2, con remuestreo a 10 m de resolución espacial\n","\n","1. B2 (Blue)\n","2. B3 (Green)\n","3. B4 (Red)\n","4. B5 (Veg. red edge)\n","5. B6 (Veg. red edge)\n","6. B7 (Veg. red edge)\n","7. B8 (NIR)\n","8. B8A (Narrow NIR)\n","9. B11 (SWIR)\n","10. B12 (SWIR)\n","\n","La estructura de directorios es el siguiente.\n","\n","+-- [nombre_proyecto]\n","  +-- shapes: shapes de aoi y capas de superposición\n","  +-- models: Almacena el modelo\n","  +-- sources: imágenes satelitales de entrada\n","  +-- results: resultados de las clasificación\n","  +-- logs: logs del entrenamiento\n","  +-- figures: imágenes y figuras "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["path_project = \"./hayuelos/\"\n","path_sources = os.path.join(path_project, \"sources\")\n","path_shapes = os.path.join(path_project, \"shapes\")\n","path_results = os.path.join(path_project, \"results\")\n","path_logs = os.path.join(path_project, \"logs\")\n","path_figures = os.path.join(path_project, \"figures\")\n","path_models = os.path.join(path_project, \"models\")\n","list_paths = [path_results, path_figures, path_models, path_logs]\n","\n","# Crear directorios temporales\n","\n","for path in list_paths:  \n","    try:\n","        os.mkdir(path)\n","    except FileExistsError:\n","        print(\"Directorio ya existe: \", os.path.basename(path))\n","\n","# Defina la imagen de entrada \n","img_train = '20210309T152639_20210309T152638_T18NWL.tif'\n","img_file = os.path.join(path_sources, img_train) \n","aoi_file = os.path.join(path_shapes, 'aoi.geojson') # Formato geográfico\n","manzana_file = os.path.join(path_shapes, 'manzana.geojson') # Formato geográfico\n","img_name = img_train.split('_')[0] \n","\n","# Cargar la imagen como un objeto Raster Dataframe y el shape de aoi como Geodaataframe\n","stack = Raster(img_file)\n","training = gpd.read_file(aoi_file)\n","manzana = gpd.read_file(manzana_file)\n","\n","bandsio = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12']\n","bandsio_names = ['B2 (Blue)', 'B3 (Green)', 'B4 (Red)', 'B5 (Veg. red edge)', \n","               'B6 (Veg. red edge)', 'B7 (Veg. red edge)', 'B8 (NIR)', \n","               'B8A (Narrow NIR)', 'B11 (SWIR)', 'B12 (SWIR)']\n","\n","# Mapea los nombres de bandas de entrada\n","[stack.rename({name: bandsio[idx]}, in_place=True) for idx, name in enumerate(stack.names)]\n","extent = es.rio.plot.plotting_extent(stack)\n",""]},{"cell_type":"markdown","metadata":{},"source":[" ## Se define la selección de bandas de entrenamineto y salida para todo el proceso"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bandsout = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B11', 'B12']\n","bandsout_names = ['B2 (Blue)', 'B3 (Green)', 'B4 (Red)', 'B5 (Veg. red edge)', \n","               'B6 (Veg. red edge)', 'B7 (Veg. red edge)', 'B8 (NIR)', \n","               'B8A (Narrow NIR)', 'B11 (SWIR)', 'B12 (SWIR)']\n",""]},{"cell_type":"markdown","metadata":{},"source":["## Anáisis exploratorio de datos\n","\n","Despliegue de imágenes y análisis exploratorio"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Despliegue los canales  imágenes\n","ep.plot_bands(stack[bandsout].read(), title=bandsout_names, figsize=(10, 8))\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Histogramas por bandas\n","\n","colors_list = [\"Blue\", \"Green\", \"Red\", \"Salmon\", \"Tomato\", \"Coral\", \"Orangered\",\n","               \"Chocolate\",\"Darkorange\",\"Maroon\"]\n","\n","ep.hist(stack[bandsout].read(), colors=colors_list, title=bandsout_names, \n","        ylabel='Reflectancia', bins=50, cols=2)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Despliegue composición de color\n","\n","rgb432 = stack[['B4','B3', 'B2']].read()\n","rgb843 = stack[['B8','B4', 'B3']].read()\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n","\n","ep.plot_rgb(rgb432, ax=ax1, title='Despliegue verdadero color (RGB432)', stretch=True)\n","ep.plot_rgb(rgb843, ax=ax2, title='Despliegue falso color (RGB843)', stretch=True)\n","plt.tight_layout()\n","plt.show()\n",""]},{"cell_type":"markdown","metadata":{},"source":[" Despliegue de las áreas de entrenamiento"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(9, 9))\n","ep.plot_rgb(rgb843, ax=ax, stretch=True, extent=extent)\n","manzana.boundary.plot(ax=ax, color='grey', alpha=0.5,)\n","training.plot(column=\"label\", cmap='RdYlGn', ax=ax, legend=True,  alpha=0.65, categorical=True)\n","ax.axis('on')\n","plt.title('Áreas de entrenamiento')\n","plt.show()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Convertir los pixeles a un dataframe de pandas\n","\n","df = stack[bandsout].to_pandas()\n","df.head()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print('Las columnas son: ', df.columns)\n","print('El tamaño´ del dataframe: ', df.shape)\n","print(df[bandsout].describe().T)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Correlacción de las bandas\n","\n","correlation_data = df.iloc[:,2:].corr()\n","correlation_data.style.background_gradient(cmap='coolwarm', axis=None)  \n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Extraer información de las bandas con la áreas de entrenamiento\n","\n","df_shape = stack[bandsout].extract_vector(training)\n","\n","df_shape = df_shape.merge(\n","    right=training.loc[:, [\"label\", \"id\"]],\n","    left_on=\"geometry_idx\",\n","    right_on=\"index\",\n","    right_index=True\n",")\n","\n","df_shape = df_shape.dropna()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Descripcion de los datos para las areas de entrenamiento\n","\n","print('Total canales espectrales: ',  df_shape.columns)\n","print('Tamaño de entrenamiento: ', df_shape.shape)\n","print(df[bandsout].describe().T)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axes = plt.subplots(1,2, figsize=(10,4), sharey=True, sharex=True)\n","plt.suptitle(\"Áreas de entrenamiento\")\n","axes[0].set_title('No vegetación')\n","axes[1].set_title('Vegetación')\n","axes[0].set_xlabel('Bands')\n","axes[0].set_ylabel('Reflectance')\n","axes[1].set_xlabel('Bands')\n","sns.pointplot(data=df_shape[df_shape['id'] == 0][bandsout], ax=axes[0], \n","              scale=0.5, estimator=np.mean)\n","sns.boxplot(data=df_shape[df_shape['id'] == 0][bandsout], ax=axes[0])\n","sns.pointplot(data=df_shape[df_shape['id'] == 1][bandsout], ax=axes[1], \n","              scale=0.5, estimator=np.mean)\n","sns.boxplot(data=df_shape[df_shape['id'] == 1][bandsout], ax=axes[1])\n","axes[0].grid()\n","axes[1].grid()\n","plt.tight_layout()\n","fig.show()\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(figsize=(10,5), sharey=True, sharex=True)\n","ax.set_title('Firmas espectrales de las áreas de entrenamiento')\n","ax.set_xlabel('Bands')\n","ax.set_ylabel('Reflectance')\n","sns.pointplot(data=df_shape[df_shape['id'] == 0][bandsout], ax=ax, \n","              scale=1, estimator=np.mean, color='red')\n","sns.pointplot(data=df_shape[df_shape['id'] == 1][bandsout], ax=ax, \n","              scale=1, estimator=np.mean, color='green')\n","ax.grid()\n","fig.show()\n",""]},{"cell_type":"markdown","metadata":{},"source":["## Entrenamineto de la red neuronal\n","\n","Dividir las areas de entrenamineto y validación "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#bands = ['B2', 'B3', 'B4', 'B7']\n","\n","X = df_shape[bandsout].values\n","y = df_shape[\"id\"].values\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Logs y callbacks para el entrenamiento\n","\n","timename = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","\n","log_csv = os.path.join(path_logs, f\"log_{img_name}-{timename}.csv\")\n","model_h5 = os.path.join(path_models, f\"model_{img_name}-{timename}.h5\")\n","imgkeras_classification = os.path.join(path_results, f\"classkeras_{img_name}-{timename}.tif\")\n","imgkeras_probability = os.path.join(path_results, f\"probkeras_{img_name}-{timename}.tif\")\n","imgkneig_classification = os.path.join(path_results, f\"classkneig_{img_name}-{timename}.tif\")\n","imgkneig_probability = os.path.join(path_results, f\"probkneig_{img_name}-{timename}.tif\")\n","fig_model = os.path.join(path_figures, f\"train_{img_name}-{timename}.png\")\n","movie_classkeras = os.path.join(path_figures, f\"movie_classkeras_{timename}.gif\")\n","\n","call_save_model = tf.keras.callbacks.ModelCheckpoint(\n","    model_h5, monitor='val_loss', verbose=0, save_best_only=False,\n","    save_weights_only=False, mode='auto', save_freq='epoch', options=None\n",")\n","\n","call_tensorboard = tf.keras.callbacks.TensorBoard(\n","    log_dir='./logs', histogram_freq=1, write_graph=True, write_images=True,\n","    update_freq='epoch', profile_batch=2, embeddings_freq=0,\n","    embeddings_metadata=True\n",")\n","\n","call_csv = CSVLogger(log_csv, separator=\",\", append=False)\n","\n","call_remote = RemoteMonitor(\n","    root='http://localhost:9000',\n",")\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Crear la funcion model de keras\n","\n","def create_keras_model(layer1_units, layer2_units, dropout_rate, l2_regularization):\n","    model = Sequential([\n","        ## Adiciona capas una por una\n","        Dense(units=layer1_units, activation='relu', input_shape=(len(bandsout),)),\n","        # Adding dropout to prevent overfitting (regularización)\n","        Dropout(dropout_rate), # 10% out in each epoc\n","        Dense(units=layer2_units, activation='relu'),\n","        # Adding dropout to prevent overfitting (regularización)\n","        #model.add(Dropout(0.1))\n","        Dense(units=1, activation='sigmoid')\n","    ])\n","    \n","    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n","    print(model.summary())\n","    return model\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Realizar el entrenamiento\n","\n","classifier_keras = KerasClassifier(\n","    build_fn=create_keras_model, \n","    batch_size=32, \n","    layer1_units=16,\n","    layer2_units=32,\n","    dropout_rate=0.025,\n","    l2_regularization=0,\n","    epochs=15, \n","    shuffle=True, \n","    validation_split=0.2,\n","    callbacks=[call_save_model, call_tensorboard, call_csv, call_remote],\n","    verbose=True\n",")\n","\n","pipeline_keras = Pipeline([\n","    ('imputer', SimpleImputer()), \n","    ('scaler', StandardScaler()), \n","    ('model', classifier_keras)\n","])\n","\n","pipeline_keras.fit(X=X_train, y=y_train)\n",""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Log CSV\n","\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n","plt.suptitle('Proceso de entrenamiento')\n","history = pd.read_csv(log_csv)\n","history[['accuracy', 'val_accuracy']].plot(ylabel='Accuracy', ax=ax1, xlabel='Epoch')\n","history[['loss', 'val_loss']].plot(ylabel='Loss', ax=ax2, xlabel='Epoch')\n","plt.tight_layout()\n","plt.show()\n","\n",""]},{"cell_type":"markdown","metadata":{},"source":["# Conclusiones\n","## Se realiza la canalización de los datos del proyecto para hacer más eficiente el entrenamiento de la imagen. \n","## En este ejemplo se muesta el wrapper de Skilearn y Keras"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}